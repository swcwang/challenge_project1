# -*- coding: utf-8 -*-
"""Copy of fin_notebook3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1YNfm_XMCFDgbl2zX_PzrMfljM6pftgh2

# Easy Peasy : Lemon Squeezy

## Objective

Explore a historical claims dataset and identify opportunities to improve the existing claims processes using Data Science Methods. Particularly:

* understand the data and identify opportunities for improvement
* demonstrate machine learning skills
* demonstrate python programming knowledge
* build a predictive model to predict the ultimate claim cost

## Import Libraries
"""

!pip install plotly

!pip install pip install dython

!pip install --upgrade scikit-learn scikit-learn-intelex --progress-bar off >> pip_installs.log

!pip install scikit-optimize

!pip install mglearn

import pandas as pd
import numpy as np
import os
from google.colab import drive
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import json
import warnings
from datetime import datetime
warnings.filterwarnings("ignore")
from sklearn import datasets
from sklearn.tree import DecisionTreeRegressor
from sklearn import tree
import matplotlib.pyplot as plt
import seaborn as sns

from scipy import stats
from scipy.stats import norm, skew
from statsmodels.stats.outliers_influence import variance_inflation_factor

from scipy.stats import boxcox
from sklearn.preprocessing import PowerTransformer, QuantileTransformer
from sklearn.model_selection import train_test_split
from sklearn.base import BaseEstimator, TransformerMixin
from sklearn.utils.validation import check_is_fitted
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import OneHotEncoder, QuantileTransformer
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.decomposition import PCA
from sklearn.compose import make_column_transformer, make_column_selector
from sklearn.pipeline import make_pipeline
from sklearn.linear_model import LinearRegression
from sklearn.compose import TransformedTargetRegressor
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import KFold
from numpy import absolute, mean
from sklearn.metrics import mean_absolute_percentage_error as MAPE, mean_absolute_error as MAE, mean_squared_error as MSE, r2_score
from sklearn.linear_model import ElasticNet
from sklearnex import patch_sklearn
patch_sklearn()

# Set sklearn display to diagrams so that pipelines can be displayed
# better
from sklearn import set_config
set_config(display = 'diagram')

"""## Loading Dataset"""

drive.mount('/content/drive/')

filename = '/content/drive/MyDrive/suncorp/archive.zip'

df_claims_ori = pd.read_csv(filename, parse_dates = ['DateTimeOfAccident', 'DateReported'])
df_claims = df_claims_ori.copy()
df_claims.head()

"""## Exploratory Data Analysis

**Goal**
1. Explore variable types
2. Dertermine dependent variable
3. Check for missing values
4. Check for outliers
5. Explore Target Distribution
6. Explore Feature Distributions
"""

pd.set_option('display.max_rows', None)

## summary descriptions
df_claims_ori.describe().T

df_claims.dtypes

"""### Preliminary Features"""

## convert "InitialIncurredClaimsCost" to a float feature
df_claims["InitialIncurredClaimsCost"] = df_claims["InitialIncurredClaimsCost"].astype('float64')

df_claims['ReportLagDays'] = ((df_claims['DateReported'] - df_claims['DateTimeOfAccident']).dt.days).astype('float64')

df_claims['ReportMonth'] = df_claims['DateReported'].dt.month.astype("int16")
df_claims['ReportYear'] = df_claims['DateReported'].dt.year.astype("int16")
df_claims['ReportDayOfWeek'] = df_claims['DateReported'].dt.dayofweek.astype("int16")

df_claims['AccidentHour'] = df_claims['DateTimeOfAccident'].dt.hour.astype("int16")
df_claims['AccidentMonth'] = df_claims['DateTimeOfAccident'].dt.month.astype("int16")
df_claims['AccidentYear'] = df_claims['DateTimeOfAccident'].dt.year.astype("int16")
df_claims['AccidentDayOfWeek'] = df_claims['DateTimeOfAccident'].dt.dayofweek.astype("int16")

target = 'UltimateIncurredClaimCost'

# this feature is basically an estimate of the target
estimate_feature = 'InitialIncurredClaimsCost'

"""### Dealing with missing values
Since the number of missing values, 29, comprises of 0.05% of the dataset, we can safely remove them
"""

df_claims.isna().sum()

df_claims = df_claims[df_claims['MaritalStatus'].notna()]

"""### Target Distribution

We'll use plotly to see the target distribution as its interactivity makes it easier to examine different regions of the plots and allows us to pick more appropriate range for displlay
"""

#@title
fig = px.histogram(df_claims, x=target,
                  marginal="box")

fig.update_layout(
    xaxis_rangeslider_visible=True, xaxis_range=[0, 50000]
)                   

fig.show()

df_claims[target].mean()

#@title
print(f"{target} has the minimum value of ${df_claims[target].min():.2f}, mean of ${df_claims[target].mean():.2f}, median of ${df_claims[target].median():.2f} and maximum of ${df_claims[target].max():,.2f}")

"""#### Dealing with outliers
Remove the highest 2% of the target (UltimateIncurredClaimCost) values, as the distribution has a long tail at the right
"""

#@title
q2 = df_claims[target].quantile(0.98)
df_claims_1 = df_claims[df_claims[target] < q2]

fig = px.histogram(df_claims_1, x=target,
                  marginal="box")
fig.update_layout(
    xaxis_rangeslider_visible=True, xaxis_range=[0, 50000]
)                
print(f"removed '{target}' values greater than {round(q2,2)} (2%)")
fig.show()

def transform_target(df):
    df[f"{target}_sq"] = df[target]**(2)  
    df[f"{target}_sqrt"] = df[target]**(1/2)
    df[f"{target}_cubrt"] = df[target]**(1/3)
    df[f"{target}_exp"] = df[target]**(1/1.2)
    df[f"{target}_reciprocal"] = 1/df[target]
    df[f"{target}_log"] = np.log1p(df[target])

    pt_bx = PowerTransformer(method = 'box-cox')
    pt_yj = PowerTransformer(method = 'yeo-johnson')
    df[f"{target}_pt_boxcox"] = pt_bx.fit_transform(np.array(df[{target}]).reshape(-1,1))[:,0]    
    df[f"{target}_pt_yeoj"] = pt_yj.fit_transform(np.array(df[{target}]).reshape(-1,1))[:,0]    

    return df

df_claims = transform_target(df_claims)

#@title
def plot_distributions(col_name, new_target, description):

  plt.figure(figsize=(15, 6))

  plt.subplot(1,2,1)

  sns.distplot(new_target, fit=norm);

  # Get the fitted parameters used by the function
  (mu, sigma) = norm.fit(new_target)

  #Now plot the distribution
  plt.title(f'{col_name} distribution - {description}',fontsize=14)
  plt.legend(['Normal dist. ($\mu=$ {:.2f} and $\sigma=$ {:.2f} )'.format(mu, sigma)],
              loc='best')

  plt.axvline(mu,linewidth  = 2 ,
                  linestyle = "dashed",color = "r" ,
                  label = "mu", ymax=0.9)

  plt.subplot(1,2,2)

  stats.probplot(new_target, plot=plt)

  plt.plot()
  return

#@title
plt.figure(figsize=(15, 6))

plt.subplot(1,2,1)

plt.xlim(-5000, 50000)
sns.distplot(df_claims_1[target] , fit=norm);

# Get the fitted parameters used by the function
(mu, sigma) = norm.fit(df_claims_1[target])

#Now plot the distribution
plt.title(f'{target} distribution - removed 2% outliers',fontsize=14)
plt.legend(['Normal dist. ($\mu=$ {:.2f} and $\sigma=$ {:.2f} )'.format(mu, sigma)],
            loc='best')

plt.axvline(mu,linewidth  = 2 ,
                linestyle = "dashed",color = "r" ,
                label = "mu", ymax=0.9)

plt.text(9000,0.00009, "normalised mean:  " + "{:.2f}".format(mu), color="r", rotation="vertical")

plt.text(22000,0.00002, "normalised", color="black")

plt.subplot(1,2,2)

stats.probplot(df_claims_1[target], plot=plt)

plt.plot()

"""#### Transform target variable
Use np.log1p(), to apply log(1+x) to the target column

Here we want to apply Log Transform to the original dataset as well as to the modified dataset which has top 2% of the target (UltimateIncurredClaimCost) removed, and see which one performed better.
"""

plot_distributions(target, df_claims[f"{target}_log"], "Applying LOG")

#@title
df_claims_1[f"{target}_log"] = np.log1p(df_claims_1[target])

plot_distributions(target, df_claims_1[f"{target}_log"], "Removed 2% outliers - Applied LOG")

"""**observation**
Looks like the raw data performed better with the log transform, as right end tail is not truncated, and the QQ plot looks better at the right end as well

Let's try a few other transforms:
* Exponential
* Box Cox
* Yeo-Johnson
* Squre root
* Cube root
"""

plot_distributions(target, df_claims[f"{target}_pt_boxcox"], "Applying Box-Cox")

plot_distributions(target, df_claims[f"{target}_pt_yeoj"], "Applying Yeo-Johnson Power Transform")

plot_distributions(target, df_claims[f"{target}_exp"], "Applying Exponential Transform")

plot_distributions(target, df_claims[f"{target}_reciprocal"],  'Applying Reciprocal Transform')

"""### Estimate Feature (Initial Incurred Claim Cost) Distribution

It would be interesting to see the estimate feature as well and see if its distribution is similar to the target distribution
"""

fig = px.histogram(df_claims, x=estimate_feature,
                  marginal="box")

fig.update_layout(
    xaxis_rangeslider_visible=True, xaxis_range=[0, 50000]
)                   

fig.show()

#@title
print(f"{estimate_feature} has the minimum value of ${df_claims[estimate_feature].min():.2f}, mean of ${df_claims[estimate_feature].mean():.2f}, median of ${df_claims[estimate_feature].median():.2f} and maximum of ${df_claims[estimate_feature].max():,.2f}")

df_claims.columns

df_claims[f"{estimate_feature}_sqrt"] = df_claims[estimate_feature]**(1/2)
df_claims[f"{estimate_feature}_cubrt"] = df_claims[estimate_feature]**(1/3)
df_claims[f"{estimate_feature}_exp"] = df_claims[estimate_feature]**(1/1.2)
df_claims[f"{estimate_feature}_reciprocal"] = 1/df_claims[estimate_feature]
df_claims[f"{estimate_feature}_boxcox"], parameters = stats.boxcox(df_claims[estimate_feature])
df_claims[f"{estimate_feature}_log"] = np.log1p(df_claims[estimate_feature])

plot_distributions(estimate_feature, df_claims[f"{estimate_feature}_log"],  f'{estimate_feature} - Log')

plot_distributions(estimate_feature, df_claims[f"{estimate_feature}_boxcox"],  f'{estimate_feature} - Box-Cox')

"""### Compare Estimate Feature with Target"""

#@title
plt.figure(figsize=(10, 10))

sns.jointplot( x = estimate_feature, y = target, data = df_claims)

"""Applying Log Transform"""

#@title
plt.figure(figsize=(20, 20))

df_claims[f"{estimate_feature}_log"] = np.log1p(df_claims[estimate_feature])

sns.jointplot( x = f"{estimate_feature}_log", y = f"{target}_log", data = df_claims)
plt.plot()

"""### Time based features

Let's remove some outliers before
"""

q5 = df_claims[target].quantile(0.95)
q10 = df_claims[target].quantile(0.9)
df_claims_1b = df_claims[df_claims[target] < q5]
df_claims_1c = df_claims[df_claims[target] < q10]

df_claims_1c["ReportYearMonth"] = df_claims_1c['DateReported'].dt.to_period('M')
df_claims_1c["AccidentYearMonth"] = df_claims_1c['DateTimeOfAccident'].dt.to_period('M')

df_claims["ReportYearMonth"] = df_claims['DateReported'].dt.to_period('M')
df_claims["AccidentYearMonth"] = df_claims['DateTimeOfAccident'].dt.to_period('M')

df_claims_1["ReportYearMonth"] = df_claims_1['DateReported'].dt.to_period('M')
df_claims_1["AccidentYearMonth"] = df_claims_1['DateTimeOfAccident'].dt.to_period('M')

#@title
plt.style.use('seaborn-white')
fig = plt.figure(figsize=(18, 6))
ax1 = plt.subplot(1,1, 1)  
ax1.set_title(f'Median "{target}" over Reported time', size=20)
df_claims_date=df_claims.groupby('ReportYearMonth').agg(['median']).reset_index()
df_claims_date.plot(x='ReportYearMonth', y=target, kind="line", ax=ax1)
plt.show()

#@title
plt.style.use('seaborn-white')
fig = plt.figure(figsize=(18, 6))
ax1 = plt.subplot(1,1, 1)  
ax1.set_title(f'Median "{target}" over Reported time - remove the upper 10%', size=20)
df_claims_1c_date=df_claims_1c.groupby('ReportYearMonth').agg(['median']).reset_index()
df_claims_1c_date.plot(x='ReportYearMonth', y=target, kind="line", ax=ax1)
plt.show()

#@title
plt.style.use('seaborn-white')
fig = plt.figure(figsize=(18, 6))
ax1 = plt.subplot(1,1, 1)  
ax1.set_title(f'Median "{estimate_feature}" over Reported time', size=20)
df_claims_1c_date=df_claims_1c.groupby('ReportYearMonth').agg(['median']).reset_index()
df_claims_1c_date.plot(x='ReportYearMonth', y=estimate_feature, kind="line", ax=ax1)
plt.show()

#@title
plt.style.use('seaborn-white')
fig = plt.figure(figsize=(18, 6))
ax1 = plt.subplot(1,1, 1)  
ax1.set_title(f'Median "{target}" over Accident time', size=20)
df_claims_1c_date=df_claims_1c.groupby('AccidentYearMonth').agg(['median']).reset_index()
df_claims_1c_date.plot(x='AccidentYearMonth', y=target, kind="line", ax=ax1)
plt.show()

#@title
plt.style.use('seaborn-white')
fig = plt.figure(figsize=(18, 6))
ax1 = plt.subplot(1,1, 1)  
ax1.set_title(f'Median "{estimate_feature}" over Accident time', size=20)
df_claims_1c_date=df_claims_1c.groupby('AccidentYearMonth').agg(['median']).reset_index()
df_claims_1c_date.plot(x='AccidentYearMonth', y=estimate_feature, kind="line", ax=ax1)
plt.show()

"""**comment**
It seems the Ultimate Claim Cost has increased over the years, however this is not reflected in the Initail Incurred Claim Cost as much. It may be worthwhile to adjust the estimate to better match the future outcome.
"""

df_claims_1c['PercentageDeviation'] = df_claims_1c['UltimateIncurredClaimCost'] / df_claims_1c['InitialIncurredClaimsCost'] * 100

#@title
plt.style.use('seaborn-white')
fig = plt.figure(figsize=(18, 6))
ax1 = plt.subplot(1,1, 1)  
ax1.set_title(f'Median "PercentageDeviation" over Accident time', size=20)
df_claims_1c_date=df_claims_1c.groupby('AccidentYearMonth').agg(['median']).reset_index()
df_claims_1c_date.plot(x='AccidentYearMonth', y='PercentageDeviation', kind="line", ax=ax1)
plt.show()

dayname = ["mon", "tue", "wed", "thur", "fri", "sat", "sun"]

#@title
fig = plt.figure(figsize=(18, 10))
ax = sns.boxplot(x="AccidentDayOfWeek", y="UltimateIncurredClaimCost", data=df_claims_1c)
plt.axhline(df_claims_1c['UltimateIncurredClaimCost'].median(),linewidth  = 1,linestyle = "dashed",color = "grey", label = "Mean")
plt.title("Accident Ultimate Claim Amount by Day of the Week")
ax.set_xticklabels(dayname)
plt.plot()

"""**observation**
There are less accidents on the weekend, but the claim amounts are higher
"""

def additionalTimeFeatures(df):
    df["AccidentDayOfWeekName"] = df["AccidentDayOfWeek"].map({0: 'Mon', 1: 'Tue', 2: 'Wed', 3: 'Thur', 4: 'Fri', 5: 'Sat', 6: 'Sun'})
    df["AccidentWeekendFlag"] = df["AccidentDayOfWeek"].apply(lambda x: "Y" if x > 4 else "N")
    df["AccidentSeason"] = df["AccidentMonth"].map({12: "Summer", 1: "Summer", 2: "Summer", 
                                                    3: "Autumn", 4: "Autumn", 5: "Autumn",
                                                    6: "Winter", 7: "Winter", 8: "Winter",
                                                    9: "Spring", 10: "Spring", 11: "Spring"})
    df["AccidentDayTimeFlag"] = df["AccidentHour"].apply(lambda x: "Y" if (x >= 6 and x <= 20) else "N")
    return df

df_claims = additionalTimeFeatures(df_claims)
df_claims_1b = additionalTimeFeatures(df_claims_1b)
df_claims_1c = additionalTimeFeatures(df_claims_1c)

df_claims.AccidentDayTimeFlag.value_counts()

x=21
if x>= 6 and x <= 20:
  print(1)
else:
  print(2)

df_claims[['AccidentHour', 'AccidentDayTimeFlag']]

pd.set_option('display.max_columns', None)
df_claims.tail()



#@title
fig = plt.figure(figsize=(18, 10))
ax = sns.boxplot(x="AccidentMonth", y="UltimateIncurredClaimCost", data=df_claims_1c)
plt.axhline(df_claims_1c['UltimateIncurredClaimCost'].median(),linewidth  = 1,linestyle = "dashed",color = "grey", label = "Mean")
plt.title("Accident Ultimate Claim Amount by Month")
#ax.set_xticklabels(dayname)
plt.plot()

#@title
fig = plt.figure(figsize=(18, 10))
ax = sns.boxplot(x="AccidentYear", y="UltimateIncurredClaimCost", data=df_claims_1c)
plt.axhline(df_claims_1c['UltimateIncurredClaimCost'].median(),linewidth  = 1,linestyle = "dashed",color = "grey", label = "Mean")
plt.title("Accident Ultimate Claim Amount by Year")
#ax.set_xticklabels(dayname)
plt.plot()

#@title
fig = plt.figure(figsize=(18, 10))
ax = sns.boxplot(x="AccidentHour", y="UltimateIncurredClaimCost", data=df_claims_1c)
plt.axhline(df_claims_1c['UltimateIncurredClaimCost'].median(),linewidth  = 1,linestyle = "dashed",color = "grey", label = "Mean")
plt.title("Accident Ultimate Claim Amount by Hour")
#ax.set_xticklabels(dayname)
plt.plot()

#@title
fig = plt.figure(figsize=(16, 8))
ax = sns.boxplot(x="AccidentSeason", y="UltimateIncurredClaimCost", data=df_claims_1c)
plt.axhline(df_claims_1c['UltimateIncurredClaimCost'].median(),linewidth  = 1,linestyle = "dashed",color = "grey", label = "Mean")
plt.title("Accident Ultimate Claim Amount by Accident Season")
#ax.set_xticklabels(dayname)
plt.plot()

#@title
fig = plt.figure(figsize=(16, 8))
ax = sns.boxplot(x="AccidentWeekendFlag", y="UltimateIncurredClaimCost", data=df_claims_1c)
plt.axhline(df_claims_1c['UltimateIncurredClaimCost'].median(),linewidth  = 1,linestyle = "dashed",color = "grey", label = "Mean")
plt.title("Accident Ultimate Claim Amount by Accident Weekend Flag")
#ax.set_xticklabels(dayname)
plt.plot()

#@title
fig = plt.figure(figsize=(16, 8))
ax = sns.boxplot(x="AccidentDayTimeFlag", y="UltimateIncurredClaimCost", data=df_claims_1c)
plt.axhline(df_claims_1c['UltimateIncurredClaimCost'].median(),linewidth  = 1,linestyle = "dashed",color = "grey", label = "Mean")
plt.title("Accident Ultimate Claim Amount by Day Time Flag")
#ax.set_xticklabels(dayname)
plt.plot()

#@title
df_claims_dow_hour = df_claims_1c.groupby(['AccidentDayOfWeek', 'AccidentHour'])['UltimateIncurredClaimCost'].median()
df_claims_dow_hour =  pd.DataFrame(df_claims_dow_hour).reset_index()
series_names = dayname

fig = px.line(df_claims_dow_hour, x='AccidentHour', y='UltimateIncurredClaimCost',
              color="AccidentDayOfWeek")

for idx, name in enumerate(series_names):
    fig.data[idx].showlegend = True
    fig.data[idx].name = name    
    fig.data[idx].hovertemplate = name
    
fig.update_layout(title=f'{target} by Accident Hour - for Day of the Week', width=1600)

fig.update_xaxes(dtick=1)

fig.show()

#@title
df_claims_dow_hour = df_claims_1c.groupby(['AccidentWeekendFlag', 'AccidentHour'])['UltimateIncurredClaimCost'].median()
df_claims_dow_hour =  pd.DataFrame(df_claims_dow_hour).reset_index()
series_names = dayname

fig = px.line(df_claims_dow_hour, x='AccidentHour', y='UltimateIncurredClaimCost',
              color="AccidentWeekendFlag")

for idx, name in enumerate(['Weekday', 'Weekend']):
    fig.data[idx].showlegend = True
    fig.data[idx].name = name    
    fig.data[idx].hovertemplate = name
    
fig.update_layout(title=f'{target} by Accident Hour - for Day of the Week', width=1600)

fig.update_xaxes(dtick=1)

fig.show()

month_names = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Aug', 'Nov', 'Dec']

#@title
df_claims_month_dow = df_claims_1c.groupby(['AccidentDayOfWeek', 'AccidentMonth'])['UltimateIncurredClaimCost'].median()
df_claims_month_dow =  pd.DataFrame(df_claims_month_dow).reset_index()

series_names = dayname

fig = px.line(df_claims_month_dow, color='AccidentDayOfWeek', y='UltimateIncurredClaimCost',
              x="AccidentMonth")

for idx, name in enumerate(series_names):
    fig.data[idx].showlegend = True
    fig.data[idx].name = name    
    fig.data[idx].hovertemplate = name
    
fig.update_layout(title=f'{target} by Accident Month - for each Day of the Week', width=1600)

fig.update_xaxes(dtick=1)

fig.show()

#@title
df_claims_month_dow = df_claims_1c.groupby(['AccidentWeekendFlag', 'AccidentMonth'])['UltimateIncurredClaimCost'].median()
df_claims_month_dow =  pd.DataFrame(df_claims_month_dow).reset_index()

series_names = dayname

fig = px.line(df_claims_month_dow, color='AccidentWeekendFlag', y='UltimateIncurredClaimCost',
              x="AccidentMonth")

for idx, name in enumerate(['Weekday', 'Weekend']):
    fig.data[idx].showlegend = True
    fig.data[idx].name = name    
    fig.data[idx].hovertemplate = name
    
fig.update_layout(title=f'{target} by Accident Month - Weekday vs Weekend', width=1600)

fig.update_xaxes(dtick=1)

fig.show()

"""**observation**
* Sunday has the highest claim amount, across all months
* Followed by Saturday
* then Monday
* there are some fluctuation between different months for each day of the week
"""



"""### Categorising Features"""

numeric_float_features = list(df_claims.select_dtypes("float64").columns)
numeric_int_features = list(df_claims.select_dtypes("int64").columns)

cat_features = list(df_claims.select_dtypes("object").columns) 
datetime_features = list(df_claims.select_dtypes("datetime64[ns, UTC]").columns)

# we want to interpret the datetime components as categorical features instead of numeric
cat_features = cat_features +list(df_claims.select_dtypes("int16").columns)

# remove some features calculated during EDA from float features
list_to_remove = [target, 'UltimateIncurredClaimCost_sqrt',
       'UltimateIncurredClaimCost_cubrt', 'UltimateIncurredClaimCost_exp',
       'UltimateIncurredClaimCost_reciprocal',
       'UltimateIncurredClaimCost_boxcox',
       'UltimateIncurredClaimCost_pt_boxcox',
       'UltimateIncurredClaimCost_pt_yeoj', 'UltimateIncurredClaimCost_quant',
       'UltimateIncurredClaimCost_log', 'InitialIncurredClaimsCost_sqrt',
       'InitialIncurredClaimsCost_cubrt', 'InitialIncurredClaimsCost_exp',
       'InitialIncurredClaimsCost_reciprocal',
       'InitialIncurredClaimsCost_boxcox', 'InitialIncurredClaimsCost_quant',
       'InitialIncurredClaimsCost_log', 'UltimateIncurredClaimCost_sq']

numeric_float_features = list(set(numeric_float_features) - set(list_to_remove))

numeric_features = numeric_float_features + numeric_int_features
cat_features.remove('ClaimNumber')

numeric_float_features

cat_features

"""### Correlation Plots"""

import plotly.figure_factory as ff
import plotly.io as pio
from dython.nominal import associations

"""#### Categorical"""

df_claims = df_claims.reset_index()

"""Use Theil's U, the Uncertainty Coefficient, to measure of nominal association between categorical variables. ie. given the value of x, how many possible states does y have, and how often do they occur. """

associations(df_claims[cat_features + [target]], nom_nom_assoc='theil', figsize=(15, 15))

"""target has some correlation with 
* Accident Year
* Marital Status
* Gender
* Accident Hour
* Accident Weekend Flag
* Part time / Full time

Highly correlated features
* Accident Day of Week and Weedkend Flag


Perhaps adding interaction variables
* Hour and Day of Week
* 


Perhaps try Chi Square as well

Accident Hour
Accident Day of Week
Part time Full time

#### Float Features
"""

#@title
pio.templates.default = "none"
corr = df_claims[numeric_float_features+ [target]].corr()

mask = np.triu(np.ones_like(corr, dtype=np.bool))

corr = corr.mask(mask)

title = 'Correlation Matrix for Float Features' 

layout = go.Layout(title_text=title, title_x=0.5, 
                   width=1000, height=1000,
                   xaxis_showgrid=False,
                   yaxis_showgrid=False,
                   yaxis_autorange='reversed')

fig = go.Figure(data =
     go.Heatmap(x = corr.index.values, y = corr.columns.values, z = corr.values, colorscale='RdBu',
                  zmax=1, zmin=-1), layout=layout)
fig = fig.update_traces(text=corr.values, texttemplate="%{text:.2f}", hovertemplate=None)


fig.update_layout(xaxis_automargin= True, yaxis_automargin = True)

fig.show()

"""**observation**
Features that are correlated with target:
* Initial Incurred Claim Amount (est feature)
* Weekend Wages
* Claim Desc Keyword 1, 6, 0, 11, 7, 3,

#### Int Features
"""

#@title
pio.templates.default = "none"
corr = df_claims[numeric_int_features+ [target]].corr('spearman')

mask = np.triu(np.ones_like(corr, dtype=np.bool))

corr = corr.mask(mask)

title = 'Correlation Matrix for Integer Features (Spearman)' 

layout = go.Layout(title_text=title, title_x=0.5, 
                   width=600, height=600,
                   xaxis_showgrid=False,
                   yaxis_showgrid=False,
                   yaxis_autorange='reversed')

fig = go.Figure(data =
     go.Heatmap(x = corr.index.values, y = corr.columns.values, z = corr.values, colorscale='RdBu',
                  zmax=0.5, zmin=-0.5), layout=layout)
fig = fig.update_traces(text=corr.values, texttemplate="%{text:.2f}", hovertemplate=None)

fig.update_layout(xaxis_automargin= True, yaxis_automargin = True)

fig.show()

#@title
pio.templates.default = "none"
corr = df_claims[numeric_int_features+ [target]].corr()

mask = np.triu(np.ones_like(corr, dtype=np.bool))

corr = corr.mask(mask)

title = 'Correlation Matrix for Integer Features (Pearson)' 

layout = go.Layout(title_text=title, title_x=0.5, 
                   width=600, height=600,
                   xaxis_showgrid=False,
                   yaxis_showgrid=False,
                   yaxis_autorange='reversed')

fig = go.Figure(data =
     go.Heatmap(x = corr.index.values, y = corr.columns.values, z = corr.values, colorscale='RdBu',
                  zmax=0.5, zmin=-0.5), layout=layout)
fig = fig.update_traces(text=corr.values, texttemplate="%{text:.2f}", hovertemplate=None)

fig.update_layout(xaxis_automargin= True, yaxis_automargin = True)

fig.show()

"""**observation**
Target has correlation with
* Age
* Number of Dependent Children
* Number of Other dependents
* Report Lag Days

#### Features of Interest

Float Features
* Initial Incurred Claim Amount (est feature)
* Weekly Wages
* Claim Desc Keyword 1, 6, 0, 11, 7, 3

Int Features
* Age
* Number of Dependent Children
* Number of Other dependents
* Report Lag Days

Categorical Features
* Accident Year
* Marital Status
* Gender
* Accident Hour
* Accident Weekend Flag
* Part time / Full time
"""

selected_features1 = [ 'Age',
 'Gender',
 'MaritalStatus',
 'DependentChildren',
 'DependentsOther',
 'WeeklyWages',
 'PartTimeFullTime',
 'ClaimDescriptionKeyword_0',
 'ClaimDescriptionKeyword_1',
 'ClaimDescriptionKeyword_2',
 'ClaimDescriptionKeyword_3',
 #'ClaimDescriptionKeyword_4',
 #'ClaimDescriptionKeyword_5',
 'ClaimDescriptionKeyword_6',
 'ClaimDescriptionKeyword_7',
 #'ClaimDescriptionKeyword_8',
 #'ClaimDescriptionKeyword_9',
 #'ClaimDescriptionKeyword_10',
 'ClaimDescriptionKeyword_11',
 'InitialIncurredClaimsCost',
 'ReportLagDays',
# 'ReportMonth',
# 'ReportYear',
# 'ReportDayOfWeek',
 'AccidentHour',
 'AccidentMonth',
 'AccidentYear',
# 'AccidentDayOfWeekName',
 'AccidentWeekendFlag',
# 'AccidentSeason',
# 'AccidentDayTimeFlag'
 ]



"""### Probability Distributions"""

cat_features

import math

"""#### Integer Features"""

#@title
df_claims['Age_brackets'] = pd.cut(x=df_claims['Age'], bins=[12, 18, 25, 30, 40, 50, 60, 70, 90])
df_claims['Age_brackets'] = df_claims['Age_brackets'].astype('str')
df_claims.Age.min(), df_claims.Age.max()
num_int_f = numeric_int_features.copy()
num_int_f.remove('Age')
num_int_f

#@title
fig = make_subplots(math.ceil(len(num_int_f)/3), cols=3,
                    subplot_titles=num_int_f)
    
for i, colname in enumerate(num_int_f):
    r= i//3 + 1
    c= i%3 + 1
    fig.add_trace(
        go.Histogram(x=df_claims[colname],
                     histnorm = "percent",
                     name = colname),
        row=r, col=c
    )


#fig.update_layout(barmode='group', bargap=0.30,bargroupgap=0.0)
fig.update_layout(height=450, width=1200, bargap=0.30,
                  title_text="Integer Feature Distributions")
                  #xaxis_tickmode = 'linear',
                  #xaxis_dtick = 1)
fig.update_yaxes(ticksuffix="%")                  
fig.update_xaxes(dtick=1)
fig.show()

#@title
fig = make_subplots(math.ceil(len(num_int_f)/3), cols=3,
                    subplot_titles=num_int_f)
    
for i, colname in enumerate(num_int_f):
    r= i//3 + 1
    c= i%3 + 1
    fig.add_trace(
        go.Box(x=df_claims[colname],
                     y=df_claims[target],
                     name = colname),
        row=r, col=c
    )


#fig.update_layout(barmode='group', bargap=0.30,bargroupgap=0.0)
fig.update_layout(height=450, width=1200, bargap=0.30,
                  title_text="Integer Feature Distributions")
                  #xaxis_tickmode = 'linear',
                  #xaxis_dtick = 1)
fig.update_yaxes(tickprefix="$")                  
fig.update_xaxes(dtick=1)
fig.update_yaxes(range=[0, 30000])  

fig.show()

#@title
fig = make_subplots(2, cols=2)

fig.add_trace(
        go.Histogram(x=df_claims['Age_brackets'],
                     histnorm = "percent",
                     name = colname),
        row=1, col=1
)

fig.add_trace(
        go.Box(x=df_claims['Age_brackets'],
                     y=df_claims[target],
                     name = colname),
        #layout_yaxis_range=[0, 50000],
        row=1, col=2
)
fig.update_xaxes(categoryorder='array', 
                 categoryarray= ['(12, 18]', '(18, 25]', '(25, 30]', '(30, 40]','(40, 50]',  '(50, 60]', 
        '(60, 70]', '(70, 90]'])

fig.update_yaxes(range=[0, 30000], row=1, col=2)  

fig.update_layout(height=500, width=1200, bargap=0.30,
                  title_text="Age Brackets")

"""#### Categorical Feature Distributions"""

#@title
cat_features_sel = ['Gender',
 'MaritalStatus',
 'PartTimeFullTime',
 'AccidentYear',
 'AccidentMonth',
 'AccidentDayOfWeekName',
 'ReportDayOfWeek',
 'AccidentWeekendFlag',
 'AccidentSeason',
 'AccidentHour',
 'AccidentDayTimeFlag',
 'ReportMonth']

#@title
fig = make_subplots(math.ceil(len(cat_features_sel)/2), cols=2,
                    subplot_titles=cat_features_sel)
    
for i, colname in enumerate(cat_features_sel):
    r= i//2 + 1
    c= i%2 + 1
    fig.add_trace(
        go.Box(x=df_claims[colname],
                     y=df_claims[target],
                     name = colname),
        #layout_yaxis_range=[0, 50000],
        row=r, col=c
    )


#fig.update_layout(barmode='group', bargap=0.30,bargroupgap=0.0)
fig.update_layout(height=1600, width=800, bargap=0.30,
                  title_text="Categorical Feature Distributions"),
                  #y_axis_range=[0, 50000]
                  #xaxis_tickmode = 'linear',
                  #xaxis_dtick = 1)
#fig.update_yaxes(ticksuffix="%")  
#fig.update_yaxis(range=[0,500000])        
fig.update_yaxes(range=[0, 20000])     
fig.update_yaxes(range=[0, 40000], row=1, col=4)     
fig.update_xaxes(dtick=1)
fig.show()

#@title
fig = make_subplots(math.ceil(len(cat_features_sel)/2), cols=2,
                    subplot_titles=cat_features_sel)
    
for i, colname in enumerate(cat_features_sel):
    r= i//2 + 1
    c= i%2 + 1
    fig.add_trace(
        go.Histogram(x=df_claims[colname],
                     histnorm = "percent",
                     name = colname),
        row=r, col=c
    )


#fig.update_layout(barmode='group', bargap=0.30,bargroupgap=0.0)
fig.update_layout(height=1800, width=800, bargap=0.30,
                  title_text="Categorical Feature Distributions")
                  #xaxis_tickmode = 'linear',
                  #xaxis_dtick = 1)
fig.update_yaxes(ticksuffix="%")                  
fig.update_xaxes(dtick=1)
fig.show()

"""#### Float Feature Distributions"""

#@title
numeric_float_features_ordered = ['ReportLagDays',
 'InitialIncurredClaimsCost',
 'HoursWorkedPerWeek',
 'WeeklyWages',
 'ClaimDescriptionKeyword_0', 
 'ClaimDescriptionKeyword_1',
 'ClaimDescriptionKeyword_2',
 'ClaimDescriptionKeyword_3',
 'ClaimDescriptionKeyword_4',
 'ClaimDescriptionKeyword_5',
 'ClaimDescriptionKeyword_6',
 'ClaimDescriptionKeyword_7',
 'ClaimDescriptionKeyword_8', 
 'ClaimDescriptionKeyword_9',
 'ClaimDescriptionKeyword_10',
 'ClaimDescriptionKeyword_11',
 ]



#@title
plt.rcParams["figure.figsize"] = (24,16)
df_claims[numeric_float_features_ordered].hist()
plt.plot()

"""**observation**
Some of the 
"""

_keywords = df_claims[numeric_float_features_ordered].head()

_keywords = df_claims.loc[:, df_claims.columns.str.contains('ClaimDescriptionKeyword_')]
_multiplot = pd.concat([_keywords, df_claims['UltimateIncurredClaimCost']], axis = 1).melt(id_vars = target)

numeric_float_features

selected_floats = ['InitialIncurredClaimsCost', 'ReportLagDays', 'WeeklyWages', 'HoursWorkedPerWeek', f'{target}_log']

target

df_claims[df_claims[target]> 500000]

# Check the skew of all numerical features
skewed_feats = df_claims[numeric_float_features].apply(lambda x: skew(x.dropna())).sort_values(ascending=False)
print("\nSkew in numerical features: \n")
skewness = pd.DataFrame({'Skew' :skewed_feats})

skewness = skewness[abs(skewness.Skew) > 0.5]
print("There are {} skewed numerical features to Box Cox transform".format(skewness.shape[0]))
skewness

#@title
def transform_skewed_features(df):
    _df = df.copy()
    log_list = ['InitialIncurredClaimsCost', 
                 'HoursWorkedPerWeek',                  ]
#                  target]
#                'ClaimDescriptionKeyword_1',
#                 'ClaimDescriptionKeyword_0',
#                 'ClaimDescriptionKeyword_8',
#                 'ClaimDescriptionKeyword_3',
#                 'ClaimDescriptionKeyword_6']
    yeoj_list = ['WeeklyWages',
                 'ReportLagDays',
                 'ClaimDescriptionKeyword_1',
                 'ClaimDescriptionKeyword_0',
                 'ClaimDescriptionKeyword_8',
                 'ClaimDescriptionKeyword_3',
                 'ClaimDescriptionKeyword_6']
    
    transformed_list = []

    pt_yj = PowerTransformer(method = 'yeo-johnson')

    for col in log_list:
        _df[f"{col}_log"] = np.log1p(_df[col])
        transformed_list.append(f"{col}_log")

    for col in yeoj_list:
        _df[f"{col}_yj"] = pt_yj.fit_transform(np.array(_df[{col}]).reshape(-1,1))[:,0] 
        transformed_list.append(f"{col}_yj")

    return _df, transformed_list
df_claims_sk1, trans_list = transform_skewed_features(df_claims)
df_claims, trans_list = transform_skewed_features(df_claims)
selected_floats = ['InitialIncurredClaimsCost_log',
 'HoursWorkedPerWeek_log',
 'WeeklyWages_yj',
 'ReportLagDays_yj', f'{target}_log']
trans_list

df_claims_sk1, trans_list = transform_skewed_features(df_claims)

df_claims, trans_list = transform_skewed_features(df_claims)

selected_floats = ['InitialIncurredClaimsCost_log',
 'HoursWorkedPerWeek_log',
 'WeeklyWages_yj',
 'ReportLagDays_yj', f'{target}_log']

trans_list

# view distributions of variables
plt.rcParams["figure.figsize"] = (16,16)
df_claims_sk1[trans_list].hist()
plt.plot()

sns.set(style="ticks", color_codes=True)

g = sns.pairplot(df_claims[selected_floats])


#import matplotlib.pyplot as plt
plt.show()



skewed_features = skewness.index

from scipy.special import boxcox1p
skewed_features = skewness.index
lam = 0.15

df_claims_sk = df_claims.copy()
for feat in skewed_features:    
    df_claims_sk[feat] = boxcox1p(df_claims[feat], lam)

skewed_feats2 = df_claims_sk[skewed_features].apply(lambda x: skew(x.dropna())).sort_values(ascending=False)
print("\nSkew in numerical features: \n")
skewness2 = pd.DataFrame({'Skew' :skewed_feats2})
skewness2

_keywords = df_claims.loc[:, df_claims.columns.str.contains('ClaimDescriptionKeyword_')]
_multiplot = pd.concat([_keywords, df_claims['UltimateIncurredClaimCost']], axis = 1).melt(id_vars = target)
_multiplot['variable'] = _multiplot['variable'].str.lstrip("ClaimDescriptionKeyword_")
_multiplot = _multiplot.rename(columns = {'variable': 'ClaimDescriptionKeyword'})

g = sns.FacetGrid(_multiplot, col = 'ClaimDescriptionKeyword', col_wrap = 4)
g.set(yscale ='log')
g.map(sns.histplot, 'value', target)
g.map(sns.regplot, 'value', target, scatter = False, line_kws = {'color': 'black'})
plt.show()

#@title
selected_floats2 = ['InitialIncurredClaimsCost_log',
 'HoursWorkedPerWeek',
 'WeeklyWages',
 'ReportLagDays']

#@title
fig = make_subplots(math.ceil(len(selected_floats2)/2), cols=2,
                    subplot_titles=selected_floats2)
    
for i, colname in enumerate(selected_floats2):
    r= i//2 + 1
    c= i%2 + 1
    fig.add_trace(
        go.Scatter(x=df_claims[colname],
                     y=df_claims[f'{target}_log'],
                     mode='markers',
                     name = colname),
        row=r, col=c
    )


#fig.update_layout(barmode='group', bargap=0.30,bargroupgap=0.0)
fig.update_layout(height=450, width=1200, bargap=0.30,
                  title_text="Integer Feature Distributions")
                  #xaxis_tickmode = 'linear',
                  #xaxis_dtick = 1)
#fig.update_yaxes(ticksuffix="%")                  
fig.update_xaxes(dtick=1)
fig.show()

for i, colname in enumerate(selected_floats2):
  fig = px.scatter(
      df_claims, x=colname, y=f'{target}_log', opacity=0.65,
      trendline='ols', trendline_color_override='darkblue', width=600
  )
  fig.show()

"""## Modelling"""

class MarriageFixNA(BaseEstimator, TransformerMixin):
    def __init__(self):
        pass
    def fit(self, X=None, y=None, **fit_params):
        return self
    def transform(self, data):
        X = data.copy()
        X["MaritalStatus"] = X["MaritalStatus"].fillna('U')
        return X

class DroppingColumns(BaseEstimator, TransformerMixin):
    def __init__(self, cols=[]):
        self.cols = cols
    def fit(self, X=None, y=None, **fit_params):
        return self
    def transform(self, data):
        X = data.copy()
        X = X.drop(self.cols,axis=1)
        return X


class CreateFeatures(BaseEstimator, TransformerMixin):
    def __init__(self):
        return None

    def fit(self, X=None, y=None, **fit_params):
        return self

    def transform(self, data):  
        X = data.copy()
        X["InitialIncurredClaimsCost"] = X["InitialIncurredClaimsCost"].astype('float64')
        X['ReportLagDays'] = ((X['DateReported'] - X['DateTimeOfAccident']).dt.days).astype('float64')
        X['AccidentHour'] = X['DateTimeOfAccident'].dt.hour.astype("int16")
        X['AccidentMonth'] = X['DateTimeOfAccident'].dt.month.astype("int16")
        X['AccidentYear'] = X['DateTimeOfAccident'].dt.year.astype("int16")
        X['AccidentDayOfWeek'] = X['DateTimeOfAccident'].dt.dayofweek.astype("int16")
        X["AccidentDayOfWeekName"] = X["AccidentDayOfWeek"].map({0: 'Mon', 1: 'Tue', 2: 'Wed', 3: 'Thur', 4: 'Fri', 5: 'Sat', 6: 'Sun'})
        X["AccidentWeekendFlag"] = X["AccidentDayOfWeek"].apply(lambda x: "Y" if x > 4 else "N")
        X["AccidentDayTimeFlag"] = X["AccidentHour"].apply(lambda x: "Y" if (x >= 6 and x <= 20) else "N")        
        return X


class SkewedFeatureTransformer(BaseEstimator, TransformerMixin):
    def __init__(self, log_list=[], yeoj_list=[]):
        self.log_list = log_list
        self.yeoj_list = yeoj_list

    def fit(self, X=None, y=None, **fit_params):
        return self

    def transform(self, data):  
        X = data.copy()
        pt_yj = PowerTransformer(method = 'yeo-johnson')

        for col in self.log_list:
            print(col)
            X[f"{col}_log"] = np.log1p(X[col])
        

        for col in self.yeoj_list:
            print(col)
            X[f"{col}_yj"] = pt_yj.fit_transform(np.array(X[{col}]).reshape(-1,1))[:,0] 

        X = X.drop(columns = self.log_list)
        X = X.drop(columns = self.yeoj_list)
        return X


class ColumnsSelector(BaseEstimator, TransformerMixin):
    # initializer 
    def __init__(self, columns):
        # save the features list internally in the class
        self.columns = columns
        
    def fit(self, X, y = None):
        return self
    def transform(self, X, y = None):
        # return the dataframe with the specified features
        return X[self.columns]

"""### Split Data"""

X_train, X_test, y_train, y_test = train_test_split(df_claims_ori.drop([target, 'ClaimNumber'], axis = 1),
                                        df_claims_ori[target], random_state = 3331)

cols_to_drop = ['DateTimeOfAccident', 'DateReported']

log_list = ['InitialIncurredClaimsCost', 'WeeklyWages', 'Age', 'HoursWorkedPerWeek']
yeoj_list = ['ReportLagDays',
            'ClaimDescriptionKeyword_1',
            'ClaimDescriptionKeyword_0',
            'ClaimDescriptionKeyword_8',
            'ClaimDescriptionKeyword_3',
            'ClaimDescriptionKeyword_6']

ohe = OneHotEncoder(handle_unknown = 'ignore', sparse=False)
categorical_cols = make_column_selector(dtype_include = object)

"""### Model 0 - Base"""

categorical_columns = list(df_claims.select_dtypes("float64").columns)
numeric_int_features = list(df_claims.select_dtypes("int64").columns)

categorical_columns = ['Gender', 'MaritalStatus', 'PartTimeFullTime']
numerical_columns = ['WeeklyWages', 'HoursWorkedPerWeek', 'Age',
 'DependentChildren',
 'DependentsOther',
 'DaysWorkedPerWeek',
 'InitialIncurredClaimsCost', 
 'ClaimDescriptionKeyword_0',
 'ClaimDescriptionKeyword_1',
 'ClaimDescriptionKeyword_2',
 'ClaimDescriptionKeyword_3',
 'ClaimDescriptionKeyword_4',
 'ClaimDescriptionKeyword_5',
 'ClaimDescriptionKeyword_6',
 'ClaimDescriptionKeyword_7',
 'ClaimDescriptionKeyword_8',
 'ClaimDescriptionKeyword_9',
 'ClaimDescriptionKeyword_10',
 'ClaimDescriptionKeyword_11']

preprocessor = make_column_transformer(
    (OneHotEncoder(drop="if_binary"), categorical_columns),
    remainder="passthrough",
    verbose_feature_names_out=False,  # avoid to prepend the preprocessor names
)

model0 = make_pipeline(
    DroppingColumns(cols=cols_to_drop),
    preprocessor,
    TransformedTargetRegressor(
        regressor=LinearRegression(), func=np.log1p, inverse_func=np.expm1
    ),
)

model0.fit(X_train, y_train)

from sklearn.metrics import median_absolute_error

y_pred0 = model0.predict(X_train)

mae = median_absolute_error(y_train, y_pred0)
mape = MAPE(y_train, y_pred0)
print(f"MAE on training set: ${mae:.2f}")
print(f"MAPE on training set: {mape:.2f}")

y_pred0 = model0.predict(X_test)
mae = median_absolute_error(y_test, y_pred0)
mape = MAPE(y_test, y_pred0)
print(f"\nMAE on testing set: {mae:.2f}")
print(f"MAPE on testing set: {mape:.2f}")

feature_names = model0[1:-1].get_feature_names_out()

coefs = pd.DataFrame(
    model0[-1].regressor_.coef_,
    columns=["Coefficients"],
    index=feature_names,
)

coefs

X_train_preprocessed = pd.DataFrame(
    model0[1:-1].transform(X_train), columns=feature_names
)

X_train_preprocessed.std(axis=0).plot.barh(figsize=(9, 7))
plt.title("Feature ranges")
plt.xlabel("Std. dev. of feature values")
plt.subplots_adjust(left=0.3)

coefs = pd.DataFrame(
    model0[-1].regressor_.coef_ * X_train_preprocessed.std(axis=0),
    columns=["Coefficient importance"],
    index=feature_names,
)
coefs.plot(kind="barh", figsize=(9, 7))
plt.xlabel("Coefficient values corrected by the feature's std. dev.")
plt.title("Linear Regression model, small regularization")
plt.axvline(x=0, color=".5")
plt.subplots_adjust(left=0.3)

"""### Model 1 - Linear Regression (Baseline)"""

additional_cols_to_drop =  ['ClaimDescriptionKeyword_4',
 'ClaimDescriptionKeyword_5',
 'ClaimDescriptionKeyword_9',
 'ClaimDescriptionKeyword_10']

preprocessor_base = make_column_transformer(
    (ohe, categorical_cols),
    remainder = 'passthrough'
)

# pipeline1 - Baseline
pipeline1 = Pipeline(steps=[ 
    ("marriage_na_fix", MarriageFixNA()),                                  
    ('create features', CreateFeatures()),
    ("drop_columns", DroppingColumns(cols=cols_to_drop+additional_cols_to_drop)),    
    ('transform skew features', SkewedFeatureTransformer(log_list=log_list, yeoj_list=yeoj_list)),
    ('preprocess', preprocessor_base),
    ('regr', LinearRegression(fit_intercept = True))  # allows line of best fit
])

pipeline1.fit(X_train, y_train)

y_pred1 = pipeline1.predict(X_test)

"""###### tranform target"""

model1 = TransformedTargetRegressor(regressor = pipeline1,
                                  func=np.log1p, 
                                  inverse_func=np.expm1)
model1.fit(X_train, y_train)

y_pred1_targ_trf =  model1.predict(X_test)

print("Baseline - Linear Regression")
print("Mean Abolute Percent Error:", round(MAPE(y_test, y_pred1), 3))
print("Mean Abolute Percent Error (Target Tranformed): ", round(MAPE(y_test, y_pred1_targ_trf), 3))

print("R squared: {}".format(r2_score(y_true=y_test,y_pred=y_pred1_targ_trf)))

_df = pd.DataFrame({'coef': pipeline1['regr'].coef_.ravel()})
_df['variable'] = pipeline1['preprocess'].get_feature_names_out()


fig, ax = plt.subplots(figsize = (8, 8))
sns.barplot(
    data = _df, x = 'coef', y = 'variable'
)
fig.tight_layout()
plt.show()

"""#### Test Residue"""

residuals = y_test-y_pred1_targ_trf
mean_residuals = np.mean(residuals)
print("Mean of Residuals {}".format(mean_residuals))

list(X_test[estimate_feature][:10])

"""#### Test Homoscedasticity"""

p = sns.scatterplot(y_pred1_targ_trf,residuals)
plt.xlabel('y_pred/predicted values')
plt.ylabel('Residuals')
#plt.ylim(-10,10)
#plt.xlim(0,26)
p = sns.lineplot([0,26],[0,0],color='blue')
p = plt.title('Residuals vs fitted values plot for homoscedasticity check')

plt.scatter(y_test, y_pred1_targ_trf, color = 'r')
plt.ylabel('Model Predictions')
plt.xlabel('True (ground truth)')

fig = px.scatter(
    df_claims, x=estimate_feature, y=target, opacity=0.65,
    trendline='ols', trendline_color_override='darkblue'
)
fig.show()

"""### Model 2 - Linear Regression - PCA

Add PCA of the Claims Description Variables
"""

pca = PCA()
pca_cols = make_column_selector(pattern = '^ClaimDescriptionKeyword_')

preprocessor = make_column_transformer(
    (pca, pca_cols),   
    (ohe, categorical_cols),
    remainder = 'passthrough'
)

pca_cols = make_column_selector(pattern = '^ClaimDescriptionKeyword_')

pipeline2 = Pipeline(steps=[ 
    ("marriage_na_fix", MarriageFixNA()),                                  
    ('create features', CreateFeatures()),
    ("drop_columns", DroppingColumns(cols=cols_to_drop)),    
    ('transform skew features', SkewedFeatureTransformer(log_list=log_list, yeoj_list=yeoj_list)), #yeoj_list=yeoj_list)),        
    ('preprocess', preprocessor),
    ('regr', LinearRegression(fit_intercept = True))
])

pipeline2.fit(X_train, y_train)

y_pred2 = pipeline2.predict(X_test)

"""#### Tranform Target"""

model2 = TransformedTargetRegressor(regressor = pipeline2,
                                  func=np.log1p, 
                                  inverse_func=np.expm1)

model2.fit(X_train, y_train)

y_pred2_targ_trf = model2.predict(X_test)

print("Baseline - Linear Regression with PCA")
print("Mean Abolute Percent Error:", round(MAPE(y_test, y_pred2), 3))
print("Mean Abolute Percent Error (Target Tranformed): ", round(MAPE(y_test, y_pred2_targ_trf), 3))

print("R squared: {}".format(r2_score(y_true=y_test,y_pred=y_pred2_targ_trf)))

"""### Model 3 - Gradient Boosting Regressor with PCA"""

pipeline3 = Pipeline(steps=[ 
    ("marriage_na_fix", MarriageFixNA()),                                  
    ('create features', CreateFeatures()),
    ("drop_columns", DroppingColumns(cols=cols_to_drop)),    
    ('transform skew features', SkewedFeatureTransformer(log_list=log_list, yeoj_list=yeoj_list)), #yeoj_list=yeoj_list)),        
    ('preprocess', preprocessor),
    ('regr', GradientBoostingRegressor())
])

model3 = TransformedTargetRegressor(regressor = pipeline3,
                                  func=np.log1p, 
                                  inverse_func=np.expm1)

model3.fit(X_train, y_train)

y_pred3_targ_trf = model3.predict(X_test)

print("Model 3 - Gradient Boosting Regressor with PCA")
print("Mean Abolute Percent Error (Target Tranformed): ", round(MAPE(y_test, y_pred3_targ_trf), 3))

print("R squared: {}".format(r2_score(y_true=y_test,y_pred=y_pred3_targ_trf)))

y_test[10:20]

round(MAPE(y_test, X_test[estimate_feature]), 3)